{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ogbn-arxiv. Number of graphs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "root = 'data'\n",
    "name = 'ogbn-arxiv'\n",
    "dataset = PygNodePropPredDataset(name, root= root, transform=T.ToSparseTensor())\n",
    "\n",
    "print(f\"Dataset: {name}. Number of graphs: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from a standard dataset in pytorch_geometric, here the Data attribute x is not the COO connectivity, rather the feature matrix X. The connectivity is in the adj_t variable. So there are a total of 169343 nodes, each one with a 128-dimensional feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.adj_t = data.adj_t.to_symmetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "None\n",
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=2315598])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([     0,      1,      2,  ..., 169145, 169148, 169251]),\n",
       " 'valid': tensor([   349,    357,    366,  ..., 169185, 169261, 169296]),\n",
       " 'test': tensor([   346,    398,    451,  ..., 169340, 169341, 169342])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dict = dataset.get_idx_split() # built-in OGB splitter\n",
    "split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = split_dict['train']\n",
    "train_idx.to(device) # also the indices are tensors\n",
    "val_idx = split_dict['valid']\n",
    "test_idx = split_dict['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make it parametric, so that it will instantiate the number of layers based on a number passed as input. Thie following schematize the model:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, act_fn = F.relu, batch_norm = True, dropout = 0, return_embeddings = False) -> None:\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.batch_norm = batch_norm\n",
    "        self.act_fn = act_fn\n",
    "        self.dropout = dropout\n",
    "        self.prediction_head = torch.nn.LogSoftmax(dim=1)\n",
    "        self.return_embeddings = return_embeddings\n",
    "        \n",
    "        # Initialize the convolution layers\n",
    "        self.conv_layers = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)]) #first layer\n",
    "        for _ in range(num_layers-2):\n",
    "            self.conv_layers.append(GCNConv(hidden_dim, hidden_dim)) #hidden layers\n",
    "        self.conv_layers.append(GCNConv(hidden_dim, output_dim)) #last layer\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.batch_units = nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "            \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.conv_layers:\n",
    "            conv.reset_parameters()\n",
    "        if self.batch_norm:\n",
    "            for b in self.batch_units:\n",
    "                b.reset_parameters()\n",
    "                \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        for l in range(len(self.conv_layers) - 1):\n",
    "            x = F.dropout(self.act_fn(self.batch_units[l](self.conv_layers[l](x, edge_index))), p = self.dropout, training = True)\n",
    "        \n",
    "        # Final layer\n",
    "        out = self.conv_layers[-1](x, edge_index) # Embeddings\n",
    "        if not self.return_embeddings:\n",
    "            out = self.prediction_head(out)  # Classification\n",
    "        \n",
    "        return out       \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(data.num_features,\n",
    "            hidden_dim = 256,\n",
    "            output_dim = dataset.num_classes,\n",
    "            num_layers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> GCN(\n",
      "  (prediction_head): LogSoftmax(dim=1)\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): GCNConv(128, 256)\n",
      "    (1-2): 2 x GCNConv(256, 256)\n",
      "    (3): GCNConv(256, 40)\n",
      "  )\n",
      "  (batch_units): ModuleList(\n",
      "    (0-2): 3 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "1 -> LogSoftmax(dim=1)\n",
      "2 -> ModuleList(\n",
      "  (0): GCNConv(128, 256)\n",
      "  (1-2): 2 x GCNConv(256, 256)\n",
      "  (3): GCNConv(256, 40)\n",
      ")\n",
      "3 -> GCNConv(128, 256)\n",
      "4 -> SumAggregation()\n",
      "5 -> Linear(128, 256, bias=False)\n",
      "6 -> GCNConv(256, 256)\n",
      "7 -> SumAggregation()\n",
      "8 -> Linear(256, 256, bias=False)\n",
      "9 -> GCNConv(256, 256)\n",
      "10 -> SumAggregation()\n",
      "11 -> Linear(256, 256, bias=False)\n",
      "12 -> GCNConv(256, 40)\n",
      "13 -> SumAggregation()\n",
      "14 -> Linear(256, 40, bias=False)\n",
      "15 -> ModuleList(\n",
      "  (0-2): 3 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "16 -> BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "17 -> BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "18 -> BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    }
   ],
   "source": [
    "for i, module in enumerate(model.modules()):\n",
    "    print(i, '->', module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OGB has also an Evaluator to assess the prformance of the model. We first compute the metrics as if Evaluator would not exist. Then, we also use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/anatali/miniconda3/envs/graphnets/lib/python3.12/site-packages/torch_sparse/tensor.py:574: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403246168/work/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  return torch.sparse_csr_tensor(rowptr, col, value, self.sizes())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-6.4012, -7.7873, -9.2531,  ..., -2.9236, -7.7304, -7.4354],\n",
       "        [-4.5846, -4.7426, -4.4914,  ..., -3.1740, -3.9634, -3.4469],\n",
       "        [-3.4448, -6.9034, -4.9559,  ..., -3.2877, -5.9135, -5.0332],\n",
       "        ...,\n",
       "        [-4.1622, -4.5818, -4.1953,  ..., -3.2897, -3.8775, -3.8222],\n",
       "        [-4.1722, -4.8866, -4.8454,  ..., -3.1790, -4.7373, -4.2510],\n",
       "        [-3.8608, -4.8809, -5.2945,  ..., -2.7713, -3.9164, -4.1986]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model(data.x, data.adj_t)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.,  7., 31.,  ..., 22., 31.,  8.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.argmax(dim=-1).float() # predictions with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5, 28,  ..., 10,  4,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.squeeze().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-16.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.nll_loss(y_preds.argmax(dim=-1).float(), data.y.squeeze().long())\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now actually train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.160, Train: 1.1755%, Valid: 1.2651%\n",
      "Epoch: 1, Loss: 3.365, Train: 17.1111%, Valid: 8.7419%\n",
      "Epoch: 2, Loss: 2.849, Train: 35.2965%, Valid: 32.3971%\n",
      "Epoch: 3, Loss: 2.524, Train: 44.3782%, Valid: 46.0888%\n",
      "Epoch: 4, Loss: 2.297, Train: 49.8180%, Valid: 52.2232%\n",
      "Epoch: 5, Loss: 2.126, Train: 53.2114%, Valid: 54.8508%\n",
      "Epoch: 6, Loss: 1.993, Train: 55.5030%, Valid: 56.4918%\n",
      "Epoch: 7, Loss: 1.889, Train: 57.2206%, Valid: 57.9046%\n",
      "Epoch: 8, Loss: 1.806, Train: 58.5555%, Valid: 58.9751%\n",
      "Epoch: 9, Loss: 1.736, Train: 59.5881%, Valid: 60.1228%\n",
      "Epoch: 10, Loss: 1.675, Train: 60.6162%, Valid: 60.8779%\n",
      "Epoch: 11, Loss: 1.622, Train: 61.3607%, Valid: 61.5960%\n",
      "Epoch: 12, Loss: 1.577, Train: 61.9380%, Valid: 62.1766%\n",
      "Epoch: 13, Loss: 1.538, Train: 62.5427%, Valid: 62.5994%\n",
      "Epoch: 14, Loss: 1.502, Train: 63.0068%, Valid: 63.1229%\n",
      "Epoch: 15, Loss: 1.469, Train: 63.4554%, Valid: 63.3947%\n",
      "Epoch: 16, Loss: 1.438, Train: 64.0074%, Valid: 63.7303%\n",
      "Epoch: 17, Loss: 1.408, Train: 64.5100%, Valid: 64.1095%\n",
      "Epoch: 18, Loss: 1.382, Train: 64.9916%, Valid: 64.3981%\n",
      "Epoch: 19, Loss: 1.358, Train: 65.3743%, Valid: 64.6666%\n",
      "Epoch: 20, Loss: 1.337, Train: 65.6953%, Valid: 64.7438%\n",
      "Epoch: 21, Loss: 1.318, Train: 66.0736%, Valid: 65.0659%\n",
      "Epoch: 22, Loss: 1.300, Train: 66.4167%, Valid: 65.4384%\n",
      "Epoch: 23, Loss: 1.283, Train: 66.7488%, Valid: 65.8143%\n",
      "Epoch: 24, Loss: 1.266, Train: 66.9698%, Valid: 66.0257%\n",
      "Epoch: 25, Loss: 1.252, Train: 67.2645%, Valid: 66.3210%\n",
      "Epoch: 26, Loss: 1.238, Train: 67.4943%, Valid: 66.5761%\n",
      "Epoch: 27, Loss: 1.225, Train: 67.7065%, Valid: 66.8076%\n",
      "Epoch: 28, Loss: 1.213, Train: 67.9210%, Valid: 67.1063%\n",
      "Epoch: 29, Loss: 1.201, Train: 68.1013%, Valid: 67.3546%\n",
      "Epoch: 30, Loss: 1.189, Train: 68.3014%, Valid: 67.4888%\n",
      "Epoch: 31, Loss: 1.178, Train: 68.4598%, Valid: 67.6097%\n",
      "Epoch: 32, Loss: 1.168, Train: 68.6786%, Valid: 67.7439%\n",
      "Epoch: 33, Loss: 1.159, Train: 68.8007%, Valid: 67.8311%\n",
      "Epoch: 34, Loss: 1.149, Train: 68.9227%, Valid: 67.9318%\n",
      "Epoch: 35, Loss: 1.140, Train: 69.0415%, Valid: 68.0426%\n",
      "Epoch: 36, Loss: 1.132, Train: 69.1371%, Valid: 68.1701%\n",
      "Epoch: 37, Loss: 1.124, Train: 69.2834%, Valid: 68.2741%\n",
      "Epoch: 38, Loss: 1.116, Train: 69.3889%, Valid: 68.3412%\n",
      "Epoch: 39, Loss: 1.109, Train: 69.5396%, Valid: 68.4285%\n",
      "Epoch: 40, Loss: 1.101, Train: 69.6902%, Valid: 68.6030%\n",
      "Epoch: 41, Loss: 1.094, Train: 69.8002%, Valid: 68.7305%\n",
      "Epoch: 42, Loss: 1.088, Train: 69.9146%, Valid: 68.8681%\n",
      "Epoch: 43, Loss: 1.081, Train: 70.0476%, Valid: 68.9956%\n",
      "Epoch: 44, Loss: 1.075, Train: 70.1807%, Valid: 69.0761%\n",
      "Epoch: 45, Loss: 1.069, Train: 70.2763%, Valid: 69.1735%\n",
      "Epoch: 46, Loss: 1.064, Train: 70.3555%, Valid: 69.2674%\n",
      "Epoch: 47, Loss: 1.058, Train: 70.4688%, Valid: 69.3480%\n",
      "Epoch: 48, Loss: 1.053, Train: 70.5743%, Valid: 69.4856%\n",
      "Epoch: 49, Loss: 1.047, Train: 70.6392%, Valid: 69.5997%\n",
      "Epoch: 50, Loss: 1.042, Train: 70.7459%, Valid: 69.6466%\n",
      "Epoch: 51, Loss: 1.037, Train: 70.7965%, Valid: 69.7037%\n",
      "Epoch: 52, Loss: 1.033, Train: 70.8800%, Valid: 69.7674%\n",
      "Epoch: 53, Loss: 1.028, Train: 70.9438%, Valid: 69.8178%\n",
      "Epoch: 54, Loss: 1.023, Train: 71.0571%, Valid: 69.8815%\n",
      "Epoch: 55, Loss: 1.019, Train: 71.1318%, Valid: 69.9654%\n",
      "Epoch: 56, Loss: 1.015, Train: 71.1890%, Valid: 70.0091%\n",
      "Epoch: 57, Loss: 1.011, Train: 71.2429%, Valid: 70.0191%\n",
      "Epoch: 58, Loss: 1.007, Train: 71.3034%, Valid: 70.0896%\n",
      "Epoch: 59, Loss: 1.003, Train: 71.3858%, Valid: 70.1601%\n",
      "Epoch: 60, Loss: 0.999, Train: 71.4562%, Valid: 70.1500%\n",
      "Epoch: 61, Loss: 0.995, Train: 71.5486%, Valid: 70.2742%\n",
      "Epoch: 62, Loss: 0.991, Train: 71.6234%, Valid: 70.3346%\n",
      "Epoch: 63, Loss: 0.987, Train: 71.6882%, Valid: 70.3816%\n",
      "Epoch: 64, Loss: 0.984, Train: 71.7740%, Valid: 70.4218%\n",
      "Epoch: 65, Loss: 0.980, Train: 71.8763%, Valid: 70.4352%\n",
      "Epoch: 66, Loss: 0.977, Train: 71.9324%, Valid: 70.4856%\n",
      "Epoch: 67, Loss: 0.973, Train: 72.0093%, Valid: 70.4889%\n",
      "Epoch: 68, Loss: 0.970, Train: 72.1006%, Valid: 70.5191%\n",
      "Epoch: 69, Loss: 0.967, Train: 72.1600%, Valid: 70.5426%\n",
      "Epoch: 70, Loss: 0.964, Train: 72.2457%, Valid: 70.5460%\n",
      "Epoch: 71, Loss: 0.960, Train: 72.3150%, Valid: 70.5896%\n",
      "Epoch: 72, Loss: 0.957, Train: 72.3876%, Valid: 70.6433%\n",
      "Epoch: 73, Loss: 0.954, Train: 72.4503%, Valid: 70.6634%\n",
      "Epoch: 74, Loss: 0.951, Train: 72.5140%, Valid: 70.6903%\n",
      "Epoch: 75, Loss: 0.948, Train: 72.5679%, Valid: 70.7406%\n",
      "Epoch: 76, Loss: 0.945, Train: 72.6262%, Valid: 70.7876%\n",
      "Epoch: 77, Loss: 0.942, Train: 72.6757%, Valid: 70.8111%\n",
      "Epoch: 78, Loss: 0.940, Train: 72.6955%, Valid: 70.8480%\n",
      "Epoch: 79, Loss: 0.937, Train: 72.7516%, Valid: 70.8581%\n",
      "Epoch: 80, Loss: 0.934, Train: 72.7966%, Valid: 70.9185%\n",
      "Epoch: 81, Loss: 0.931, Train: 72.8406%, Valid: 70.9520%\n",
      "Epoch: 82, Loss: 0.929, Train: 72.9176%, Valid: 70.9957%\n",
      "Epoch: 83, Loss: 0.926, Train: 72.9759%, Valid: 71.0326%\n",
      "Epoch: 84, Loss: 0.924, Train: 73.0276%, Valid: 71.0695%\n",
      "Epoch: 85, Loss: 0.921, Train: 73.0979%, Valid: 71.1366%\n",
      "Epoch: 86, Loss: 0.918, Train: 73.1650%, Valid: 71.1870%\n",
      "Epoch: 87, Loss: 0.916, Train: 73.2068%, Valid: 71.1903%\n",
      "Epoch: 88, Loss: 0.913, Train: 73.2530%, Valid: 71.2071%\n",
      "Epoch: 89, Loss: 0.911, Train: 73.3036%, Valid: 71.2440%\n",
      "Epoch: 90, Loss: 0.909, Train: 73.3443%, Valid: 71.2742%\n",
      "Epoch: 91, Loss: 0.906, Train: 73.4058%, Valid: 71.3145%\n",
      "Epoch: 92, Loss: 0.904, Train: 73.4564%, Valid: 71.3011%\n",
      "Epoch: 93, Loss: 0.902, Train: 73.4993%, Valid: 71.3480%\n",
      "Epoch: 94, Loss: 0.899, Train: 73.5598%, Valid: 71.3715%\n",
      "Epoch: 95, Loss: 0.897, Train: 73.5983%, Valid: 71.4051%\n",
      "Epoch: 96, Loss: 0.895, Train: 73.6225%, Valid: 71.4319%\n",
      "Epoch: 97, Loss: 0.892, Train: 73.6763%, Valid: 71.4923%\n",
      "Epoch: 98, Loss: 0.890, Train: 73.7302%, Valid: 71.5058%\n",
      "Epoch: 99, Loss: 0.888, Train: 73.7863%, Valid: 71.5393%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.reset_parameters()\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_preds = model(data.x, data.adj_t) # notice how here it predicts in all the nodes\n",
    "    \n",
    "    loss = F.nll_loss(y_preds[train_idx], data.y[train_idx].squeeze().long()) # only computed on the train index\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    correct_train = (y_preds[train_idx].argmax(dim=-1) == data.y[train_idx].squeeze().long()).sum().item()\n",
    "    total_train = len(train_idx)\n",
    "    accuracy_train = correct_train / total_train\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    best_val_acc = 0\n",
    "    best_model = None\n",
    "    with torch.no_grad(): # disable gradient tracking\n",
    "        model.eval() # disables dropout and batch norm training\n",
    "        # Differently from standard ML here the model has already made predictions on the validation examples, but they did not influence the loss\n",
    "        correct_val = (y_preds[val_idx].argmax(dim=-1) == data.y[val_idx].squeeze().long()).sum().item()\n",
    "        total_val = len(val_idx)\n",
    "        accuracy_val = correct_val / total_val\n",
    "        \n",
    "        if accuracy_val > best_val_acc:\n",
    "            best_val_acc = accuracy_val\n",
    "            best_model = copy.deepcopy(model)\n",
    "        \n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item():.3f}, Train: {100 * accuracy_train:.4f}%, Valid: {100 * accuracy_val:.4f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since we did not perform any hyperparameter tuning, there is no need to re-train all the model on training + validation data and then evaluate on the test data. We can simply pick the best validation score model and assess its performance in the test data, which in this case should be very similar to the validation one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Evaluator from ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer, loss_fn):\n",
    "    \n",
    "    loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = model(data.x, data.adj_t)\n",
    "    loss = loss_fn(preds[train_idx], data.y[train_idx].squeeze(dim=1).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator, save_model_results = False):\n",
    "    \n",
    "    model.eval()\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Evaluation\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "    \n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        \n",
    "        if not os.path.exists('predictions'):\n",
    "            os.makedirs('predictions')\n",
    "        \n",
    "        # Save locally as csv\n",
    "        df.to_csv('predictions/ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "        \n",
    "    return train_acc, valid_acc, test_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cpu',\n",
       " 'num_layers': 4,\n",
       " 'hidden_dim': 256,\n",
       " 'dropout': 0.5,\n",
       " 'lr': 0.01,\n",
       " 'epochs': 100}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    'device': device,\n",
    "    'num_layers': 4,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.01,\n",
    "    'epochs': 100,\n",
    "}\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 4.0237, Train: 23.43%, Valid: 28.00% Test: 25.10%\n",
      "Epoch: 02, Loss: 2.4252, Train: 25.02%, Valid: 20.25% Test: 18.61%\n",
      "Epoch: 03, Loss: 2.1185, Train: 32.05%, Valid: 37.66% Test: 39.43%\n",
      "Epoch: 04, Loss: 1.9246, Train: 32.77%, Valid: 38.92% Test: 42.82%\n",
      "Epoch: 05, Loss: 1.7551, Train: 38.01%, Valid: 40.87% Test: 42.14%\n",
      "Epoch: 06, Loss: 1.6082, Train: 41.60%, Valid: 40.06% Test: 39.13%\n",
      "Epoch: 07, Loss: 1.5586, Train: 41.27%, Valid: 36.81% Test: 37.90%\n",
      "Epoch: 08, Loss: 1.5155, Train: 39.47%, Valid: 32.20% Test: 34.42%\n",
      "Epoch: 09, Loss: 1.4573, Train: 38.05%, Valid: 32.63% Test: 35.73%\n",
      "Epoch: 10, Loss: 1.4120, Train: 36.23%, Valid: 31.35% Test: 35.20%\n",
      "Epoch: 11, Loss: 1.3859, Train: 34.87%, Valid: 29.61% Test: 33.31%\n",
      "Epoch: 12, Loss: 1.3622, Train: 33.51%, Valid: 27.72% Test: 30.37%\n",
      "Epoch: 13, Loss: 1.3302, Train: 31.45%, Valid: 24.87% Test: 26.00%\n",
      "Epoch: 14, Loss: 1.3092, Train: 30.85%, Valid: 23.52% Test: 24.08%\n",
      "Epoch: 15, Loss: 1.2846, Train: 31.31%, Valid: 24.68% Test: 25.89%\n",
      "Epoch: 16, Loss: 1.2670, Train: 33.32%, Valid: 26.90% Test: 28.16%\n",
      "Epoch: 17, Loss: 1.2519, Train: 36.68%, Valid: 31.95% Test: 34.48%\n",
      "Epoch: 18, Loss: 1.2345, Train: 40.27%, Valid: 38.73% Test: 41.59%\n",
      "Epoch: 19, Loss: 1.2247, Train: 43.54%, Valid: 43.19% Test: 45.35%\n",
      "Epoch: 20, Loss: 1.2122, Train: 45.35%, Valid: 44.91% Test: 46.78%\n",
      "Epoch: 21, Loss: 1.1963, Train: 47.52%, Valid: 48.26% Test: 49.93%\n",
      "Epoch: 22, Loss: 1.1893, Train: 49.56%, Valid: 49.74% Test: 51.39%\n",
      "Epoch: 23, Loss: 1.1788, Train: 51.21%, Valid: 51.09% Test: 52.53%\n",
      "Epoch: 24, Loss: 1.1687, Train: 53.60%, Valid: 53.06% Test: 54.73%\n",
      "Epoch: 25, Loss: 1.1562, Train: 56.97%, Valid: 57.38% Test: 58.55%\n",
      "Epoch: 26, Loss: 1.1498, Train: 59.05%, Valid: 59.04% Test: 60.43%\n",
      "Epoch: 27, Loss: 1.1391, Train: 61.42%, Valid: 61.66% Test: 62.52%\n",
      "Epoch: 28, Loss: 1.1340, Train: 62.13%, Valid: 62.50% Test: 63.33%\n",
      "Epoch: 29, Loss: 1.1256, Train: 62.47%, Valid: 62.67% Test: 63.12%\n",
      "Epoch: 30, Loss: 1.1167, Train: 62.63%, Valid: 62.49% Test: 63.01%\n",
      "Epoch: 31, Loss: 1.1133, Train: 63.16%, Valid: 63.11% Test: 63.30%\n",
      "Epoch: 32, Loss: 1.1078, Train: 64.04%, Valid: 64.14% Test: 64.45%\n",
      "Epoch: 33, Loss: 1.1022, Train: 64.28%, Valid: 64.61% Test: 64.51%\n",
      "Epoch: 34, Loss: 1.0918, Train: 64.81%, Valid: 64.85% Test: 64.89%\n",
      "Epoch: 35, Loss: 1.0915, Train: 65.65%, Valid: 65.63% Test: 65.62%\n",
      "Epoch: 36, Loss: 1.0864, Train: 65.21%, Valid: 64.70% Test: 65.06%\n",
      "Epoch: 37, Loss: 1.0776, Train: 65.37%, Valid: 65.27% Test: 65.11%\n",
      "Epoch: 38, Loss: 1.0762, Train: 65.67%, Valid: 65.56% Test: 65.71%\n",
      "Epoch: 39, Loss: 1.0670, Train: 66.23%, Valid: 66.37% Test: 66.12%\n",
      "Epoch: 40, Loss: 1.0669, Train: 66.62%, Valid: 66.71% Test: 66.46%\n",
      "Epoch: 41, Loss: 1.0597, Train: 66.81%, Valid: 66.64% Test: 66.73%\n",
      "Epoch: 42, Loss: 1.0561, Train: 66.93%, Valid: 66.59% Test: 66.10%\n",
      "Epoch: 43, Loss: 1.0509, Train: 67.22%, Valid: 66.29% Test: 65.59%\n",
      "Epoch: 44, Loss: 1.0476, Train: 67.22%, Valid: 66.51% Test: 65.78%\n",
      "Epoch: 45, Loss: 1.0454, Train: 67.55%, Valid: 66.35% Test: 65.72%\n",
      "Epoch: 46, Loss: 1.0405, Train: 67.65%, Valid: 66.46% Test: 66.01%\n",
      "Epoch: 47, Loss: 1.0378, Train: 67.75%, Valid: 67.02% Test: 67.13%\n",
      "Epoch: 48, Loss: 1.0337, Train: 68.21%, Valid: 67.28% Test: 67.78%\n",
      "Epoch: 49, Loss: 1.0290, Train: 68.44%, Valid: 68.28% Test: 67.94%\n",
      "Epoch: 50, Loss: 1.0265, Train: 68.66%, Valid: 68.43% Test: 67.81%\n",
      "Epoch: 51, Loss: 1.0194, Train: 68.81%, Valid: 68.04% Test: 67.74%\n",
      "Epoch: 52, Loss: 1.0182, Train: 68.91%, Valid: 67.78% Test: 67.02%\n",
      "Epoch: 53, Loss: 1.0139, Train: 68.84%, Valid: 67.68% Test: 67.37%\n",
      "Epoch: 54, Loss: 1.0134, Train: 69.23%, Valid: 68.46% Test: 67.77%\n",
      "Epoch: 55, Loss: 1.0115, Train: 69.52%, Valid: 68.34% Test: 66.88%\n",
      "Epoch: 56, Loss: 1.0094, Train: 69.71%, Valid: 68.44% Test: 67.00%\n",
      "Epoch: 57, Loss: 1.0042, Train: 69.60%, Valid: 68.76% Test: 67.90%\n",
      "Epoch: 58, Loss: 1.0001, Train: 69.73%, Valid: 68.80% Test: 68.23%\n",
      "Epoch: 59, Loss: 0.9991, Train: 69.68%, Valid: 69.18% Test: 68.73%\n",
      "Epoch: 60, Loss: 0.9958, Train: 70.13%, Valid: 69.51% Test: 68.73%\n",
      "Epoch: 61, Loss: 0.9923, Train: 70.20%, Valid: 69.49% Test: 68.66%\n",
      "Epoch: 62, Loss: 0.9943, Train: 70.22%, Valid: 69.70% Test: 68.84%\n",
      "Epoch: 63, Loss: 0.9873, Train: 70.27%, Valid: 69.55% Test: 68.88%\n",
      "Epoch: 64, Loss: 0.9885, Train: 70.18%, Valid: 69.73% Test: 69.25%\n",
      "Epoch: 65, Loss: 0.9848, Train: 70.21%, Valid: 69.69% Test: 69.21%\n",
      "Epoch: 66, Loss: 0.9817, Train: 70.40%, Valid: 69.42% Test: 68.38%\n",
      "Epoch: 67, Loss: 0.9783, Train: 70.50%, Valid: 69.07% Test: 67.43%\n",
      "Epoch: 68, Loss: 0.9810, Train: 70.30%, Valid: 68.98% Test: 67.69%\n",
      "Epoch: 69, Loss: 0.9740, Train: 70.25%, Valid: 69.20% Test: 68.16%\n",
      "Epoch: 70, Loss: 0.9721, Train: 70.28%, Valid: 69.44% Test: 68.56%\n",
      "Epoch: 71, Loss: 0.9716, Train: 70.18%, Valid: 69.24% Test: 68.24%\n",
      "Epoch: 72, Loss: 0.9689, Train: 70.41%, Valid: 69.28% Test: 68.07%\n",
      "Epoch: 73, Loss: 0.9673, Train: 70.54%, Valid: 69.37% Test: 67.98%\n",
      "Epoch: 74, Loss: 0.9628, Train: 70.76%, Valid: 69.61% Test: 68.52%\n",
      "Epoch: 75, Loss: 0.9630, Train: 70.88%, Valid: 69.54% Test: 68.30%\n",
      "Epoch: 76, Loss: 0.9611, Train: 70.93%, Valid: 69.76% Test: 68.61%\n",
      "Epoch: 77, Loss: 0.9600, Train: 70.92%, Valid: 68.97% Test: 67.08%\n",
      "Epoch: 78, Loss: 0.9574, Train: 70.87%, Valid: 68.98% Test: 67.10%\n",
      "Epoch: 79, Loss: 0.9556, Train: 70.76%, Valid: 69.85% Test: 69.20%\n",
      "Epoch: 80, Loss: 0.9526, Train: 70.91%, Valid: 70.12% Test: 69.29%\n",
      "Epoch: 81, Loss: 0.9504, Train: 71.13%, Valid: 69.57% Test: 67.72%\n",
      "Epoch: 82, Loss: 0.9486, Train: 71.07%, Valid: 70.13% Test: 69.03%\n",
      "Epoch: 83, Loss: 0.9517, Train: 71.05%, Valid: 70.29% Test: 69.82%\n",
      "Epoch: 84, Loss: 0.9455, Train: 70.90%, Valid: 69.96% Test: 69.95%\n",
      "Epoch: 85, Loss: 0.9481, Train: 71.50%, Valid: 70.09% Test: 68.91%\n",
      "Epoch: 86, Loss: 0.9409, Train: 71.44%, Valid: 70.25% Test: 68.85%\n",
      "Epoch: 87, Loss: 0.9414, Train: 71.36%, Valid: 70.46% Test: 69.93%\n",
      "Epoch: 88, Loss: 0.9397, Train: 71.38%, Valid: 70.12% Test: 69.98%\n",
      "Epoch: 89, Loss: 0.9383, Train: 71.54%, Valid: 70.53% Test: 69.35%\n",
      "Epoch: 90, Loss: 0.9397, Train: 71.24%, Valid: 70.09% Test: 68.57%\n",
      "Epoch: 91, Loss: 0.9355, Train: 71.19%, Valid: 70.28% Test: 69.50%\n",
      "Epoch: 92, Loss: 0.9315, Train: 70.97%, Valid: 69.81% Test: 69.41%\n",
      "Epoch: 93, Loss: 0.9305, Train: 71.22%, Valid: 70.39% Test: 69.76%\n",
      "Epoch: 94, Loss: 0.9287, Train: 71.40%, Valid: 70.41% Test: 69.63%\n",
      "Epoch: 95, Loss: 0.9290, Train: 71.64%, Valid: 70.24% Test: 69.77%\n",
      "Epoch: 96, Loss: 0.9260, Train: 71.64%, Valid: 70.18% Test: 69.40%\n",
      "Epoch: 97, Loss: 0.9221, Train: 71.37%, Valid: 69.45% Test: 67.79%\n",
      "Epoch: 98, Loss: 0.9235, Train: 71.68%, Valid: 70.17% Test: 68.58%\n",
      "Epoch: 99, Loss: 0.9179, Train: 71.68%, Valid: 70.53% Test: 69.67%\n",
      "Epoch: 100, Loss: 0.9224, Train: 71.90%, Valid: 70.38% Test: 69.20%\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import Evaluator\n",
    "evaluator = Evaluator(name = 'ogbn-arxiv')\n",
    "model = GCN(data.num_features, args['hidden_dim'],\n",
    "            dataset.num_classes, args['num_layers'],\n",
    "            dropout = args['dropout']).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args['lr'])\n",
    "loss_fn = F.nll_loss\n",
    "\n",
    "best_model = None\n",
    "best_valid_acc = 0\n",
    "\n",
    "for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
    "    result = test(model, data, split_dict, evaluator)\n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "            f'Loss: {loss:.4f}, '\n",
    "            f'Train: {100 * train_acc:.2f}%, '\n",
    "            f'Valid: {100 * valid_acc:.2f}% '\n",
    "            f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model Predictions\n",
      "Best model: Train: 71.45%, Valid: 70.71% Test: 69.30%\n"
     ]
    }
   ],
   "source": [
    "best_result = test(best_model, data, split_dict, evaluator, save_model_results=True)\n",
    "train_acc, valid_acc, test_acc = best_result\n",
    "print(f'Best model: '\n",
    "    f'Train: {100 * train_acc:.2f}%, '\n",
    "    f'Valid: {100 * valid_acc:.2f}% '\n",
    "    f'Test: {100 * test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
